
*** THIS DOCUMENTATION IS NOT REGULARILY CHECKED TO BE UP-TO-DATE. ***

last update: 20130724

See bwc-trace.sh and bwc-trace.m for the most up-to-date checks relative
to the exact meaning of most files.

See bwc-ptrace.sh and bwc-ptrace.m for the most up-to-date checks relative
to the exact meaning of most files for the case of linear systems over
GF(p). More generally, all GF(p) specific information is in README.gfp


The tools present in this subdirectory form an implementation of the
Block-Wiedemann algorithm with the following features:

- parallelism at a multithread / MPI level. No fully integrated
  multi-site level yet, but bare bones are there.
- checkpointing / restarting in case of problem.
- checksums during the computation to detect problems.
- possibility to compute either a left or right kernel vector.
- cpu binding (if hwloc is available)

Properly documenting bwc implies a bit of knowledge of the block Wiedemann
algorithm and its parameters. This README does not cover this aspect.

Input, output, and formats.
===========================

The input is a matrix that comes out from merge (the .small matrix). Two
input formats are supported.
 - The legacy ascii format uses one header line saying nrows,
   ncols, plus <nrows> rows, with <nentries> [<column indices of non-zero
   coefficients>].
 - The binary format is headerless. A file [PREFIX].bin is expected to
   contain nrows rows, each coded as the row length <nentries> written as
   a 32-bit little-endian integer, followed by the <nentries> indices of
   non-zero columns, all as 32-bit little-endian integers. Companion
   files named [PREFIX].rw.bin and [PREFIX].cw.bin can be present or
   generated by the tool called mf_scan. These indicates the row weights
   and column weights, always as 32-bit LE integers.

The output is a set of vectors of the matrix nullspace. Either left
nullspace (zero combination of rows) or right nullspace (zero combination
of columns) may be computed. The kernel ends up in the file called W in
the chosen working directory.


Calling sequence of binaries
============================

We describe here the list of binaries to run. This can be somewhat automated
using the bwc.pl driver script, documented below.

 1 - mf_bal
     Given files for row and column frequencies of the input matrix,
     compute a permutation of rows and columns which would provide the
     best balancing of the computation load across the different jobs.
     decomposes the input matrix into blocks that will be dispatched to
     nodes
 2 - dispatch
     apply the previous permutation to the input matrix, and compute
     in-memory caches.
 3 - prep
     select input blocks of vectors, and check that rank conditions are
     satisfied
 4 - secure
     precompute some data useful for checksums
 5 - split Y
     split the main input vector Y into several blocks for independent
     processing at different sites.
!6 - krylov
     compute the Krylov sequence
 7 - acollect
     collect the Krylov sequence files into one big file. Can be done at
     several moments in the computation.
!8 - lingen
     Berlekamp-Massey -like step: produces a generating polynomial
 9 - split F
     split the linear generator computed into several blocks for independent
     processing at each site.
!10 - mksol
     create the solutions from the polynomial
 11 - gather
     reconstruct solutions from the mess

krylov, lingen, mksol are the steps having a real computational impact.
prep is cheap but important for ensuring the success of the algorithm.
split,acollect,gather are just trivial bookkeeping.
dispatch may be I/O intensive.
secure is akin to doing a few krylov iterations. It runs for roughly just
as long as the desired time between two recoverable checkpoints (the
longer it runs, the less often checks are done -- but checks are
relatively cheap anyway). It is possible to skip this step using the
skip_online_checks option (see below).

Parameters
==========

All binaries accept command-line parameters. As per the CADO params.[ch]
conventions, several syntaxes are possible, but the current documentation
suggests only the <key>=<value> form.

Note that some of the core operations performed by binaries may depend on
the command-line specified parameters. The CADO code embarks pre-computed
code for common sets of parametsrs. Working with blocks of vectors of
width 64 utilizes code which has inlined functions specific to this
width. Programs achieve versatility w.r.t. this width by loading the
corresponding code from shared libraries. These shared libraries, named
e.g. libmatmul_u64k1_bucket.so for the default compiled library, are
expected to be present in the same directory as the calling binary.

* wdir=<directory name>

    Work in the specified directory. This implies that the program will cd
    into that directory prior to doing any disk access, so relative path might
    trigger bugs. All output will appear in the specified directory.

    Having a wdir is generally a good idea, as bwc will otherwise happily
    populate your working directory with data files.

* cfg=<filename>

    Read extra parameters from file name.

* matrix=<path>
* balancing=<path>
    path to matrix file, and to balancing file. May be absolute paths,
    otherwise understood as relative from the current directory (working
    directory). Note that the matrix cache files are named relative to
    that directory as well (unless the local_cache_copy_dir parameter is
    used). The ``matrix'' parameter may point to a remote URI in any
    format recognized by the cURL library, if the use of the latter has
    been enabled.
    
* m=<number>, n=<number>, or mn=<number> for setting both.

    Set the corresponding parameters from the block Wiedemann algorithm. Note
    that these are the _global_ values. Presumably both are multiples of 64.
    If two sites are meant to compute two sequences independently, the n value
    here is the sum of the sequence widths.

* nullspace=[left|right]

    Look for left or right kernel vectors. If one looks for a right kernel
    vector, then the matrix-times-vector operation will be performed.
    Otherwise, the vector-times-matrix operation is used, or equivalently
    transpose-of-matrix times vector.

* mpi=<number1>x<number2>

    Work on a grid of <number1>x<number2> nodes. <number1> rows, <number2>
    columns. The ``rows'' and ``columns'' receive matrix blocks according to
    their positioning.

* thr=<number1>x<number2>

    Same on the thread level. This is not exclusive of the above.

* interleaving=[0|1]

    Whether to compute _two_ sequences simultaneously. This is relevant when a
    significant amount of time is spent in matrix communications, so that
    while communication is taking place, the CPU is idle and can be put at
    work. This doubles the number of threads of the program, and increases the
    memory footprint a bit (the matrix data which can be shared is indeed
    shared).

* interval=<number>

    Check for consistency every <number> matrix product iterations. Note
    that this also governs the number of intermediary vectors which are
    stored. If the value here is too small, then there are chances that
    you exhaust your disk space.

* mm_impl=[basic|sliced|threaded|bucket|...]

    low-level matrix multiplication code to use. See below in this document.

* seed=<number>

    random seed. the ``prep'' program is not deterministic, so for debugging
    purposes, setting this parameter to a fixed value may be wise.

* lingen_threshold=<number>

    Threshold for recursive algorithm, only relevant for lingen. <number> is
    currently 64.

Some more internal parameters.

* start=<value>, end=<value> limits for doing partial krylov/mksol
* ys=<number y0>..<number y1> specify which sub-sequence to work with.
  In normal usage y0 should be 0, and y1 should be the parameter "n".
  When interleaving=1, this interval is divided into two sub-intervals
  describing two subsequences whose computation is interleaved. The
  quantity K=(y1-y0)/(1+interleaving) quantifies the operating width of the
  innermost arithmetical operations ; when K=64, operations are based on
  unsigned longs (on 64-bit machines). Usual values for K are 64 and 128.
  Note that when ys is different from 0..n, then computation only involves
  some of the needed subsequences, not all -- and all subsequences must be
  computed.
* checkpoints=[0|1] deprecated. Do not use. Note that this flag is NOT
  used for krylov/mksol, which always store checkpoints.
* splits=<number1>,<number2>,... Only for the auxilary program ``split''.
  Splits Y0-<n> (or equivalently F) in files V<number1>-<number2>.*,
  V<number2>-<number3>.*, to prepare for several independent subsequence
  computations.
* Some cpu-level parameters, relevant for the low-level routines.
    - l1_cache_size=<number>
    - l2_cache_size=<number>
    - cache_line_size=<number>
        The length of some data blocks is set in accordance with these values.
        You may benefit from tuning them, but presumably only after you've
        gained some familiarity with the relevant source code.
* mm_* : parameters passed on to low-level matrix multiplication routines.
    - mm_store_transposed=[0|1] forces whether the matrix is stored row-major
      (0) or column-major (1). The low-level implementations are written with
      a preferred ordering in mind which is set automatically, so don't change
      this unless you're willing to try out with something new.
    - other mm_xxx_* parameters are not yet documented.
* sequential_cache_build=[0|1] whether the in-memory caches must be built
  one after another (=1), or all in parallel (=0, default behaviour).
  Since this operation requires significantly more RAM than what is used
  within the actual computation, it makes sense.
* skip_online_checks=[0|1] whether krylov and mksol should check their
  vector multiples against the check data computed by the ``secure''
  step. The default is to do the check. If it is not done, it must be
  done externally (by a program which would be trivial to write but which
  incidetally has never been written).
* keep_rolling_checkpoints=<n> instructs the program to discard the
  rolling vector checkpoints every <n> checkpoints. This assumes for
  example that checkpoints are offloaded to external storage in the
  background by another process. Use of this option is conjunction with
  skip_online_checks=1 is dangerous, since it leaves open the possibility
  of a failure which stands no chance to be eventually detected.
* save_submatrices=[0|1] save the sub-matrices used by each node in
  simple binary files. These files are not used by the computation, so
  this option is not enabled by default.


Files (standard name, standard place, etc)
==========================================

This list links at who creates what and uses what. The prefix ``mat'' is
actually replaced by the basename of the input matrix, with suffixes
removed.

wdir/mat.<nh>x<v>.<bchecksum>.bin  permutations used for balancing.
    created by mf_bal
    read by dispatch primarily, but relevant to the whole computation.
wdir/mat.<nh>x<v>.<bchecksum>.hi.vj        sub-matrix files.
    created by dispatch
    unused (can be useful for debugging, though)
wdir/mat.<nh>x<v>.<bchecksum>.hi.vj-MMMMMM.bin       in-memory version of the sub-matrices
    created by {dispatch,prep,secure,krylov,mksol,gather} if not found.
    used by {prep,secure,krylov,mksol,gather}
wdir/X.<bchecksum>                  X vector used for Krylov.
    created by prep
    read by krylov and secure
wdir/Y.0.<bchecksum>                Y vector used for Krylov.
    created by prep
    read by split ifile=Y.0 ofile-fmt=V%u-%u.0 splits=[...,]<n1>,<n2>[,...]
wdir/C.<n>.<bchecksum>              check vector = trsp(M)^n * X  (See note (T))
    created by secure
    read by krylov mksol
wdir/V<n1>-<n2>.0.<bchecksum>       columns <n1>..<n2> (n2 exclusive) of Y
    created by split ifile=Y.0 ofile-fmt=V%u-%u.0 splits=[...,]<n1>,<n2>[,...]
    read by krylov mksol
wdir/A<n1>-<n2>.<j1>-<j2>       krylov sequence data.
    created by krylov
    read by acollect. acollect replaces all these files by a file matching the
    same pattern, read in turn by lingen.
wdir/V<n1>-<n2>.<j>.<bchecksum>     columns <n1>..<n2> of M^j Y (See note (T))
    created by krylov while computing.
    never read again unless for external checking (not all of it implemented
    yet)
wdir/F_INIT_QUICK               initial data for lingen
    created by lingen
    never read again unless by lingen again for resuming (resuming in lingen
    is not functional at the moment)
wdir/pi-<t1>-<t2>               current state for lingen
    created by lingen
    never read again unless by lingen again for resuming (resuming in lingen
    is not functional at the moment)
wdir/F                          linear generator.
    created by lingen
    read by split --ifile F --ofile-fmt F.sols0-$n.%u-%u
wdir/F<n1>..<n2>                columns <n1>..<n2> of F (n2 exclusive).
    created by split --ifile F --ofile-fmt F.sols0-$n.%u-%u")
    read by mksol
wdir/S<n1>-<n2>.<j>.<bchecksum>     mksol sequence data.
    created by mksol
    read by gather
wdir/W.<bchecksum>                  kernel vectors
    created by gather

The permutation P
=================

bwc now uses (optionally) a shuffled matrix times vector product, which
as a side effect of the matrix times vector product also multiplies by a
certain permutation matrix. This is done for best efficiency. For the
case of the un-shuffled product, P is the identity.

This permutation matrix is as follows. We assume that the matrix m is
split in nh horizontal and nv vertical strips. M is first and foremost
padded trivially to obtain a larger matrix Mx whose number of rows and
columns are equal, and both multiples of nh*nv, say we have
nr==nc==nh*nv*nz for an integer nz.

Each row index i in [0..nr] can be written (i*nv+j)*nz+k, 0<=i<nh,
0<=j<nv, 0<=k<nz. The image by P of this index is (j*nh+i)*nz+k (or the
opposite, see bwc_trace.m to see which is right). Magma code for
computing the corresponding permutation matrix follows:

    pr:=func<x|(qr*nh+qq)*nz+r+1 where qq,qr is Quotrem(q, nv) where q,r is
    Quotrem(x-1,nz)>;
    Pr:=PermutationMatrix(GF(2),[pr(x):x in [1..nrx]]);


Per-program documentation
=========================

For all programs, more extensive documentation can often be found in the
source file comments if there are such comments.

mf_bal
------
    mf_bal computes a row and column permutation which is applicable to
    the input matrix M, to be transformed into a twisted matrix
    Sr*M*Sc^-1 which such that when split in a 2d rectangular grid
    matching the job organization, one expect the blocks to be reasonably
    balanced in terms of number of coefficients.

    Sc is the permutation sending 0 to the first 32-bit integer found
    in col_perm, etc.  The current code enforces Sr == Sc.

    mf_bal does not actually read the input matrix, but works only with
    the row and column frequencies.

    mf_bal is not an mpi program.

    mf_bal computes a ``bchecksum'' which identifies the permutation
    being used.

    Note that as of 20110321, output files of bwc are invariant under
    this permutation. Even though internal computation uses it, this is
    transparent to the user.

dispatch
--------
    dispatch uses the mf_bal output to compute the per-job in-memory
    data.  mat.<nh>x<nv>.<bchecksum>.hi.vj denotes the (i,j) block of the
    matrix, but is normally not saved to disk. Rather, the copies
    mat.<nh>x<nv>.<bchecksum>.hi.vj-XXXXXXXX.bin of the in-memory
    structures are saved.

    <ugly>
        There is an nfs-related kludge here, which may make bwc
        inappropriate for foreign matrices (this is fixable). We rely on
        the fact that the weight of the *columns* of the input matrix
        (assuming it's stored on disk row by row) is most unbalanced, so
        that mf_bal has computed Sc first, and Sr hardly matters. We
        expect Sr==Sc. Write Mt the matrix corresponding to the local
        blocks as they are present on the nodes/cores. The ``shuffled
        product'' will actually mutiply by Mt*Pr^-1 (when multiplying on
        the left), or P^-1*Mt when multiplying on the right.

        For this reason, the blocks dispatched are not the blocks of the
        matrix Sr*M*Sc^-1, but those of the matrix Mt==P*Sr*M*Sc^-1. This
        way, we are working with iterates of the matrix P*Sc*M*(P*Sr)^-1
        for nullspace=left, and Sr*M*Sc^-1 for nullspace=right. For
        Sc==Sr, these are conjugates of M in both cases.
    </ugly>

    All vector files considered *internally* later on, and relative to
    the matrix Mt, are ``twisted'' by the permutation related to the one
    computed by mf_bal. Output vectors are not affected, they are
    invariant under the permutation. For reference, here is how the
    internal data looks like (bwc-ptrace.m is supposed to have
    authoritative check lines for this).

    In the case of nullspace=left, we have
        Ytwisted = Y * (P*Sc)^-1 -- or P*Sc*Y if written transposed.
        Ytwisted * Mt = Y * M * (P*Sr)^-1 = Y * M * Sr^-1*Sc * (P*Sc)^-1
    and for nullspace=right:
        Ytwisted = Sc * Y
        Mt * Ytwisted = Sr*M*Y = Sc * Sc^-1*Sr * M * Y.
    Thus in the case where Sc==Sr, it is possible to untwist the twisted
    files in a consistent way.

build
-----

    This program does not belong to the usual sequence of binaries. Yet,
    it is a simple program whose job is only to create the in-memory
    cache from the matrix blocks mat.hi.vj (which get created if and only
    if the option save_submatrices=1 is used); note in particular that
    creating this in-memory cache is a memory-intensive operations, which
    typically requires two to four times as much memory as the program
    will eventually use once the matrix has been treated.

    The syntax for build is:

        build <ascii file> <impl> [left|right]

    E.g.:
        build /tmp/wdir/mat.h0.v0 bucket left

    Note that the [left|right] parameter must match the nullspace parameter
    used globally for bwc.

    Currently this program computed cache for width 64 bits, but this can
    be changed trivially to another size by changing the source, or
    supporting an extra parameter.  Note also that choosing the data
    width has an impact on the cached matrix file which is created. It
    must match the width which is used eventually for krylov/mksol,
    otherwise data block sizes might be chosen too large or too small. In
    the latter case, you will encounter the warning ``cached matrix file
    fits data with different striding''. This is not innocuous for
    krylov/mksol, as it hurts performance.

    Once /tmp/wdir/mat.h0.v0.bucket.bin (or /tmp/wdir/mat.h0.v0.bucketT.bin)
    are created, /tmp/wdir/mat.h0.v0 is normally never read again.

prep
----

    prep looks for sensible starting vectors X and Y for use with the block
    Wiedemann algorithm. X is a block of m vectors, Y a block of n vectors.
    prep ensures that the rank conditions are satisfied for the first few
    iterates trsp(X)*M*Y, trsp(X)*M^2*Y, trsp(X)*M^3*Y (See note (T)).

    En route, prep often creates the in-memory cached matrix file, unless the
    build program mentioned above has been called already. If several
    independent sequences are to be computed, this is a problem, as prep will
    build a cache for n-bit wide vector entries, while krylov/mksol will in
    this case work with shorter sequences.

    X and Y are stored as:

    X.<bchecksum>                    X vector used for Krylov.
    Y.0.<bchecksum>                  Y vector used for Krylov.

secure
------

    secure reads the output from prep, and creates:
    
    C.<interval>.<bchecksum>         check vector = trsp(M)^<interval> * X
    
    When M^(k*interval)*Y has been computed, using C one can look ahead the
    value trsp(X) * M^((k+1)*interval) * Y which will be computed after
    computing M^((k+1)*interval)*Y. This helps making sure that no bit flipped
    in the computation.

    Note that this checking mechanism is incomplete as it does not provide
    off-line checking. Off-line checking is possible at moderate cost, and
    will be implemented soon.

    (See note (T) for the whole paragraphs above).

split (split Y)
---------------

    The syntax of split is ./split [--split-y|--split-f] --splits=<integer list>

    We assume below that the integer list is for example 0,64,192

    split --split-y reads only Y.0.<bchecksum>, which is a vector whose entries
    are 192-bit wide, and creates

    V0-64.0.<bchecksum>            columns 0..63 of Y.
    V64-192.0.<bchecksum>          columns 64..191 of Y.

krylov
------

    krylov produces the linear sequence (a_ij), as a set of files named:

    A<n1>-<n2>.<j1>-<j2>

    Such a file contains j2-j1 consecutive blocks of m by (n2-n1) bit
    matrices, stored in binary format, row major. 

    n1 and n2 are constant throughout the krylov run, and are specified
    with the ys=<n1>,<n2> command-line argument.

    n2-n1 determines which shared library the binary loads for performing
    its lowest-level arithmetic operations. For n2-n1==64, the shared
    library named libmatmul_u64k1_MMMMMMM.bin is loaded.
    
    The j-j1-th matrix in A<n1>-<n2>.<j1>-<j2> (for j1<=j<j2) is equal to
    trsp(X)*M^j*Y (see note (T)).

    krylov also creates
    
    V<n1>-<n2>.<j>.<bchecksum>      columns <n1>..<n2> of M^j Y. (see note (T)).

    This can be used to resume computations at iteration j.

    (j2-j1 is always equal to <interval> as in C.<interval>.<bchecksum>, and j is
    always a multiple of <interval>).

    When V<n1>-<n2>.<j2>.<bchecksum> is computed, krylov verifies that it is in
    accordance with the input vector V<n1>-<n2>.<j1>.<bchecksum>. The file
    A<n1>-<n2>.<j1>-<j2> is written only if this test succeeds. Otherwise the
    program aborts.

    Note on resuming. If the extra parameter start=<j> is passed to krylov,
    then computations resume after iteration <j>. Instead of
    V.<n1>-<n2>.0.<bchecksum>, V.<n1>-<n2>.<j>.<bchecksum> is read instead, and
    only subsequent A files will be touched. It is also possible to change the
    end value for the computation, in order to stop the subsequence
    computation prematurely.

    Note on interleaving. Selecting interleaving is functionally
    equivalent to running two instances of krylov, one with
    ys=<j1>..<jmiddle>, the other one with ys=<jmiddle>..<j2>. So
    with ys=0..128, the data width used by each instance is 64.

acollect
--------

    acollect does a trivial bookkeeping task. Consider the files
    A<n1>-<n2>.<j1>-<j2> as representing a rectangle in the space [0,<n>[ *
    [0,jmax[. acollect verifies that these rectangles do not overlap, and
    creates a unique file A0-<n>.0-<jmax>

    If the --remove-old argument is given, the newly created file replaces the
    old files.

lingen
------

    lingen computes the linear generator F from the sequence A. It
    represents an n times n matrix of polynomials of degree ceil(N/n)
    which satisfies

    A * trsp(F) = matrix of polynomials of degree < ceil(N/n)

    F is stored coefficient by coefficient. Each coefficient is an n
    times n bit matrix, stored row-major.

    Note that lingen is currently the oldest code in bwc. Some parts of
    it were written when the corresponding algorithm barely existed. It
    admittedly deserves a rewrite.

split (split F)
---------------

    in bijection with what is done for splitting Y, split --split-F
    selects only part of the columns of F. This creates:

    F<n1>..<n2>                columns <n1>..<n2> of F (n2 exclusive).

mksol
-----

    This reads the files F<n1>..<n2> as well as Y<n1>..<n2>.0.<bchecksum>,
    and creates files:

    S<n1>..<n2>.<j1>..<j2>.<bchecksum>      partial sums for evaluation.

    All the S* files must be summed to form the final kernel vector
    element.

    note: off-line checking for mksol is not implemented yet.  mksol does
    checks in the same way krylov does, and saves over the V* files as
    well, which is admittedly silly since these files are likely to exist
    already, or will never be reused anyway except for off-line checking
    (which, as said, is not implemented yet).

gather
------

    This sums all S* files, and check how many times one must iterate in
    order to obtain M^k*(sum_of_S) == 0 (see note T). Then it stores:

    W.<bchecksum>                   M^(k-1)*sum_of_S (see note T).

Using bwc.pl
============

    bwc.pl is a perl script which tries to be intelligent, and runs the
    programs above in the correct order. It does cope with multi-node MPI
    environment, but does not yet handle several independent sequences
    (except for interleaved jobs).

    The spirit is that all parameters listed above are to be given to
    bwc.pl ; the _first_ provided argument must be either the basename of
    a specific binary to call (with possibly the proper mpiexec arguments
    added, and possibly the unrecognized arguments stripped out), or one
    of the meta-commands :balance, :complete, and :wipeout.

    More extensive documentation in bwc.pl is included in the file
    header.

Using bwc.pl with MPI (updated 20120704)
=====================

    1) first you have to compile CADO-NFS with MPI support:

    $ MPI=1 make

    Check the following lines are present in the make output:

    -- Using MPI C compiler /usr/bin/mpicc
    -- Using MPI C++ compiler /usr/bin/mpic++
    -- Using MPI driver /usr/bin/mpiexec

    2) secondly you need a working directory (wdir) which is accessible by all
       machines (it can be empty initially)

    3) then launch the following command where you have to adapt:
       thr=3x4   each machine has 12 cores, we split them in 3x4
       mpi=2x2:  we use 4 machines, and split them in 2x2
       hosts=... list of machines used (number should match mpi=...)
       matrix=   location of the matrix (on the machine where bwc.pl is run)
       wdir=...  working directory (accessible by all machines)
       bwc_bindir= where the binaries are (accessible by all machines)
       mpi_extra_args= this is needed for openmpi

    $ bwc.pl :complete seed=1 thr=2x2 mpi=2x2 hosts=andouille,chipolata,chorizo,merguez matrix=/localdisk/tmp/nfs/c156.sparse.bin nullspace=left mm_impl=bucket interleaving=0 interval=1000 mn=64 wdir=/global/bwc.wdir shuffled_product=1 bwc_bindir=/global/bin/bwc mpi_extra_args='--mca btl_tcp_if_exclude lo,virbr0'

Cpu binding (updated 20141002)
===========

If hwloc (Portable Hardware Locality, http://www.open-mpi.org/projects/hwloc/)
is installed, then bwc.pl might use it for cpu binding, which will improve the
performance of the linear algebra step. When hwloc is used, the keyword
"cpubinding" should appear in cxxx.bwc.log.

See cpubinding.conf for more information. In short, you need to add
cpubinding=$(CADO)/linalg/bwc/cpubinding.conf on the bwc.pl command line.


Notes:
======

(T): This assumes that nullspace=right. If nullspace=left, replace M by trsp(M).
