N = 90377629292003121684002147101760858109247336549001090677693
name = testrun
workdir = work

# Adjust execpath to point to the root of your CADO build directory
tasks.execpath=${HOME}/build/cado-nfs
tasks.verbose = 1
tasks.lim0=50000
tasks.lim1=100000
tasks.lpb0=22
tasks.lpb1=22
tasks.mfb0=24
tasks.mfb1=24
threads=2
tasks.polyselect.verbose = 0
tasks.polyselect.P = 420
tasks.polyselect.degree = 4
tasks.polyselect.admin = 0
tasks.polyselect.admax = 10000
tasks.polyselect.adrange = 5000
# tasks.sieve.rels_wanted = 222540
tasks.sieve.rels_wanted = 122540
tasks.sieve.qrange = 2000
tasks.I = 11
tasks.sieve.lambda0=1.2
tasks.sieve.lambda1=1.2
tasks.filter.nslices_log=1
tasks.filter.keep = 160
tasks.filter.skip = 32
tasks.filter.target_density = 170
tasks.filter.maxlevel = 15
tasks.linalg.m = 64
tasks.linalg.n = 64
tasks.linalg.verbose = 0
tasks.linalg.nchar = 64

# This reads hostnames from the file specified in the shell environment 
# variable $OAR_NODE_FILE
slaves.catrel.hostnames = @${OAR_NODE_FILE}
# nrclients defines how many clients to start on each unique hostname
# Note that the total number of threads in use will be nrclient * nrthreads
slaves.catrel.nrclients = 8
# scriptpath is where the cado-nfs-client.py script can be found
slaves.catrel.scriptpath = ${HOME}/git/cado-nfs
slaves.catrel.downloadretry = 10
# This causes cadofactor to startclient by using "oarsh" for remote login
# instead of the default "ssh"
slaves.catrel.ssh.execbin = oarsh
